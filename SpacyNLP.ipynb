{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpacyNLP.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAtFWAdkObdx"
      },
      "source": [
        "import re\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5EG3oWbAPwl"
      },
      "source": [
        "#! pip install spacy\n",
        "#! python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "#nlp = spacy.load('en') # 'parser', 'tagger', 'ner'\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "from spacy import displacy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4X-LWsrOjPP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "eda0433b-aea3-484d-f5e5-79a06964d4c8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stops = stopwords.words(\"english\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPbN63zDAPww"
      },
      "source": [
        "# Importing data\n",
        "data=pd.read_excel(\"data.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8G60kBcAPwy"
      },
      "source": [
        "texts=data.content.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWTFjeVUAPw1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e47608cc-7831-403e-bf42-711cb96ab6fe"
      },
      "source": [
        "texts[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Having traveled all over the world , I've had the entire gamut of tour operators. The good, the bad and the in-between. Gardman tours and their guide Hayk Azatyan was absolutely at the top of the list. Hayk showed up on time and had a superb Land Cruiser ready for our all day off-road adventure. Hayk's knowledge is expansive and he is friendly and relaxed. We spent 11 hours (the tour is advertised as eight but he was so busy showing me his country time got away from us!) touring the canyon, seeing the pagan temple and going to beautiful ancient monasteries before driving high into the Caucasus Mountains and experiencing so much of beautiful Armenia that I cannot give a higher recommendation than to make sure you contact Gardman Tours on your trip to Armenia.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kecS3AoAPw5"
      },
      "source": [
        "## Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnUTRyaBAPw6"
      },
      "source": [
        "text1=\"I booked with Hyur the apartment and 8 tours I guess.. the apartment was near the center and very close to Hyur but needs some reparations as I already told them.. Zara was the person with whom I was communicating to book the apartment ,tours and to resolve some problems... she was very responsive.. thanks Zara for everything thing.. the staff in the office at Nalbandyan was helpful.. thanks... now for the tours ,expect long distance sometimes and bad roads but the places are splendid.. the guides Tamara and Mania were great and Tamara was always smiling and polite.. thanks for the girls a lot.. now for Tigran I canâ€™t thank him enough he is our hero ,he stayed with us until 2h oâ€™clock in the morning.. if he didnâ€™t stay and make his calls for sure we were going to sleep in the street.. that was a bad incident hope Hyur will resolve it after I talked to them.. for the restaurants that have been chosen there are some very bad especially when we were in the tour of Khor Virap,noravank..\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOw8253SAPw9"
      },
      "source": [
        "text2=\"\"\"From heavy symbolic blocks, coloured glass, lots of hares, elephants teapots and blue fish right up to the bottom of the Cascade\n",
        "The structure of the Cascade looks like a pre-Colombian pyramid\n",
        "Stairs that lead up and up, dotted with odd sculptures and levels where there are whirls, and wall sculptures, plinths and carvings\n",
        "The range of sculptures is truly nuts from a psychedelic 70's LOVE sculpture, something that could almost be Assyrian to modern\n",
        "Unfortunately the fountains were not operating - which would have made this even more spectacular\n",
        "You can also go inside where there is an escalator that takes you to a number of landings each with its own set of sculptures and more hanging from the ceilings and along the slopes by the escalator\n",
        "Some landings had entrances to museums - but by then I was overloaded with sculpture - couldn't take any more\n",
        " Each landing also has an entrance to the outside so you need not walk up the stairs or you can combine the stairs and escalator\n",
        "At the top you can actually see Mt.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H7ZvaIQRGhM"
      },
      "source": [
        "# sample text\n",
        "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. \\\n",
        "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown \\\n",
        "printer took a galley of type and scrambled it to make a type specimen book. It has survived not \\\n",
        "only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. \\\n",
        "It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, \\\n",
        "and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\\n",
        "There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration \\\n",
        "in some form, by injected humour, or randomised words which don't look even slightly believable. If you are \\\n",
        "going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the \\\n",
        "middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, \\\n",
        "making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined \\\n",
        "with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated \\\n",
        "Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65XhovhwAPxA"
      },
      "source": [
        "doc=nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZul6eW5XQlv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a75c5a21-7e24-4987-b2d2-686995a2141a"
      },
      "source": [
        "#Getting tokens and sentences from the text\n",
        "list(nlp(\"I am a data scientist. I know Python and R.\")) # Tokens\n",
        "list(nlp(\"I am a data scientist. I know Python and R.\").sents) # Sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[I am a data scientist., I know Python and R.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "029fzvpLXTaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "aae3923b-5ee0-48a7-c88e-8fbdc06a1a1e"
      },
      "source": [
        "# Token methdos\n",
        "doc = nlp(\"I'm a data scientist. I know Python and R.\")\n",
        "for token in doc:\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\t0\t-PRON-\tFalse\tFalse\tX\tPRON\tPRP\n",
            "'m\t1\tbe\tFalse\tFalse\t'x\tVERB\tVBP\n",
            "a\t4\ta\tFalse\tFalse\tx\tDET\tDT\n",
            "data\t6\tdata\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            "scientist\t11\tscientist\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            ".\t20\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
            "I\t22\t-PRON-\tFalse\tFalse\tX\tPRON\tPRP\n",
            "know\t24\tknow\tFalse\tFalse\txxxx\tVERB\tVBP\n",
            "Python\t29\tpython\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n",
            "and\t36\tand\tFalse\tFalse\txxx\tCCONJ\tCC\n",
            "R.\t40\tr.\tFalse\tFalse\tX.\tPROPN\tNNP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ1PAs4fAPxC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "b56d3ca0-b939-4de2-bc2d-8f7cb8297266"
      },
      "source": [
        "# Sentences in the text\n",
        "list(doc.sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lorem Ipsum is simply dummy text of the printing and typesetting industry.,\n",
              " Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.,\n",
              " It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.,\n",
              " It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.,\n",
              " There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable.,\n",
              " If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text.,\n",
              " All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet.,\n",
              " It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable.,\n",
              " The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PVvscKNP8G2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "bc715cc2-4773-480f-a14e-481750ed7d6f"
      },
      "source": [
        "# Named entities in the text\n",
        "doc.ents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Lorem Ipsum,\n",
              " Lorem Ipsum,\n",
              " the 1500s,\n",
              " only five centuries,\n",
              " the 1960s,\n",
              " Letraset,\n",
              " Lorem Ipsum,\n",
              " Aldus PageMaker,\n",
              " Lorem Ipsum,\n",
              " Lorem Ipsum,\n",
              " Lorem Ipsum,\n",
              " first,\n",
              " 200,\n",
              " Latin,\n",
              " Lorem Ipsum,\n",
              " Lorem Ipsum)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAtBvzWfaaZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "0e85325d-0462-4992-e0f6-1083f64b1ada"
      },
      "source": [
        "[[i.text,i.label_] for i in doc.ents]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Lorem Ipsum', 'PERSON'],\n",
              " ['Lorem Ipsum', 'PERSON'],\n",
              " ['the 1500s', 'DATE'],\n",
              " ['only five centuries', 'DATE'],\n",
              " ['the 1960s', 'DATE'],\n",
              " ['Letraset', 'ORG'],\n",
              " ['Lorem Ipsum', 'PERSON'],\n",
              " ['Aldus PageMaker', 'ORG'],\n",
              " ['Lorem Ipsum', 'PERSON'],\n",
              " ['Lorem Ipsum', 'PERSON'],\n",
              " ['Lorem Ipsum', 'PERSON'],\n",
              " ['first', 'ORDINAL'],\n",
              " ['200', 'CARDINAL'],\n",
              " ['Latin', 'NORP'],\n",
              " ['Lorem Ipsum', 'PERSON'],\n",
              " ['Lorem Ipsum', 'PERSON']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeOh1IgsQiHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 25995
        },
        "outputId": "c639703d-cb4f-4039-8ab5-e81bbbbcda73"
      },
      "source": [
        "# Prints lemmas, POS and DEP of the text\n",
        "doc.print_tree()\n",
        "\n",
        "#nlp(\"I am a data scientist.\").print_tree()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VBZ',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'be',\n",
              "  'modifiers': [{'NE': 'PERSON',\n",
              "    'POS_coarse': 'PROPN',\n",
              "    'POS_fine': 'NNP',\n",
              "    'arc': 'nsubj',\n",
              "    'lemma': 'Lorem Ipsum',\n",
              "    'modifiers': [],\n",
              "    'word': 'Lorem Ipsum'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'ADV',\n",
              "    'POS_fine': 'RB',\n",
              "    'arc': 'advmod',\n",
              "    'lemma': 'simply',\n",
              "    'modifiers': [],\n",
              "    'word': 'simply'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'NOUN',\n",
              "    'POS_fine': 'NN',\n",
              "    'arc': 'attr',\n",
              "    'lemma': 'text',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'ADJ',\n",
              "      'POS_fine': 'JJ',\n",
              "      'arc': 'amod',\n",
              "      'lemma': 'dummy',\n",
              "      'modifiers': [],\n",
              "      'word': 'dummy'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'of',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NN',\n",
              "        'arc': 'pobj',\n",
              "        'lemma': 'industry',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'DET',\n",
              "          'POS_fine': 'DT',\n",
              "          'arc': 'det',\n",
              "          'lemma': 'the',\n",
              "          'modifiers': [],\n",
              "          'word': 'the'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'NOUN',\n",
              "          'POS_fine': 'NN',\n",
              "          'arc': 'amod',\n",
              "          'lemma': 'printing',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'CCONJ',\n",
              "            'POS_fine': 'CC',\n",
              "            'arc': 'cc',\n",
              "            'lemma': 'and',\n",
              "            'modifiers': [],\n",
              "            'word': 'and'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'NOUN',\n",
              "            'POS_fine': 'NN',\n",
              "            'arc': 'conj',\n",
              "            'lemma': 'typesetting',\n",
              "            'modifiers': [],\n",
              "            'word': 'typesetting'}],\n",
              "          'word': 'printing'}],\n",
              "        'word': 'industry'}],\n",
              "      'word': 'of'}],\n",
              "    'word': 'text'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': '.',\n",
              "    'arc': 'punct',\n",
              "    'lemma': '.',\n",
              "    'modifiers': [],\n",
              "    'word': '.'}],\n",
              "  'word': 'is'},\n",
              " {'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VBN',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'be',\n",
              "  'modifiers': [{'NE': 'PERSON',\n",
              "    'POS_coarse': 'PROPN',\n",
              "    'POS_fine': 'NNP',\n",
              "    'arc': 'nsubj',\n",
              "    'lemma': 'Lorem Ipsum',\n",
              "    'modifiers': [],\n",
              "    'word': 'Lorem Ipsum'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'VERB',\n",
              "    'POS_fine': 'VBZ',\n",
              "    'arc': 'aux',\n",
              "    'lemma': 'have',\n",
              "    'modifiers': [],\n",
              "    'word': 'has'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'NOUN',\n",
              "    'POS_fine': 'NN',\n",
              "    'arc': 'attr',\n",
              "    'lemma': 'text',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NN',\n",
              "      'arc': 'poss',\n",
              "      'lemma': 'industry',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'DET',\n",
              "        'POS_fine': 'DT',\n",
              "        'arc': 'det',\n",
              "        'lemma': 'the',\n",
              "        'modifiers': [],\n",
              "        'word': 'the'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'PART',\n",
              "        'POS_fine': 'POS',\n",
              "        'arc': 'case',\n",
              "        'lemma': \"'s\",\n",
              "        'modifiers': [],\n",
              "        'word': \"'s\"}],\n",
              "      'word': 'industry'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADJ',\n",
              "      'POS_fine': 'JJ',\n",
              "      'arc': 'amod',\n",
              "      'lemma': 'standard',\n",
              "      'modifiers': [],\n",
              "      'word': 'standard'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NN',\n",
              "      'arc': 'amod',\n",
              "      'lemma': 'dummy',\n",
              "      'modifiers': [],\n",
              "      'word': 'dummy'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'since',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'ADV',\n",
              "        'POS_fine': 'RB',\n",
              "        'arc': 'advmod',\n",
              "        'lemma': 'ever',\n",
              "        'modifiers': [],\n",
              "        'word': 'ever'},\n",
              "       {'NE': 'DATE',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NNS',\n",
              "        'arc': 'pobj',\n",
              "        'lemma': 'the 1500s',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'PUNCT',\n",
              "          'POS_fine': ',',\n",
              "          'arc': 'punct',\n",
              "          'lemma': ',',\n",
              "          'modifiers': [],\n",
              "          'word': ','},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'VERB',\n",
              "          'POS_fine': 'VBD',\n",
              "          'arc': 'relcl',\n",
              "          'lemma': 'take',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'ADV',\n",
              "            'POS_fine': 'WRB',\n",
              "            'arc': 'advmod',\n",
              "            'lemma': 'when',\n",
              "            'modifiers': [],\n",
              "            'word': 'when'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'NOUN',\n",
              "            'POS_fine': 'NN',\n",
              "            'arc': 'nsubj',\n",
              "            'lemma': 'printer',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'DET',\n",
              "              'POS_fine': 'DT',\n",
              "              'arc': 'det',\n",
              "              'lemma': 'an',\n",
              "              'modifiers': [],\n",
              "              'word': 'an'},\n",
              "             {'NE': '',\n",
              "              'POS_coarse': 'ADJ',\n",
              "              'POS_fine': 'JJ',\n",
              "              'arc': 'amod',\n",
              "              'lemma': 'unknown',\n",
              "              'modifiers': [],\n",
              "              'word': 'unknown'}],\n",
              "            'word': 'printer'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'NOUN',\n",
              "            'POS_fine': 'NN',\n",
              "            'arc': 'dobj',\n",
              "            'lemma': 'galley',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'DET',\n",
              "              'POS_fine': 'DT',\n",
              "              'arc': 'det',\n",
              "              'lemma': 'a',\n",
              "              'modifiers': [],\n",
              "              'word': 'a'},\n",
              "             {'NE': '',\n",
              "              'POS_coarse': 'ADP',\n",
              "              'POS_fine': 'IN',\n",
              "              'arc': 'prep',\n",
              "              'lemma': 'of',\n",
              "              'modifiers': [{'NE': '',\n",
              "                'POS_coarse': 'NOUN',\n",
              "                'POS_fine': 'NN',\n",
              "                'arc': 'pobj',\n",
              "                'lemma': 'type',\n",
              "                'modifiers': [],\n",
              "                'word': 'type'}],\n",
              "              'word': 'of'}],\n",
              "            'word': 'galley'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'CCONJ',\n",
              "            'POS_fine': 'CC',\n",
              "            'arc': 'cc',\n",
              "            'lemma': 'and',\n",
              "            'modifiers': [],\n",
              "            'word': 'and'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'VERB',\n",
              "            'POS_fine': 'VBD',\n",
              "            'arc': 'conj',\n",
              "            'lemma': 'scramble',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'PRON',\n",
              "              'POS_fine': 'PRP',\n",
              "              'arc': 'dobj',\n",
              "              'lemma': '-PRON-',\n",
              "              'modifiers': [],\n",
              "              'word': 'it'},\n",
              "             {'NE': '',\n",
              "              'POS_coarse': 'VERB',\n",
              "              'POS_fine': 'VB',\n",
              "              'arc': 'advcl',\n",
              "              'lemma': 'make',\n",
              "              'modifiers': [{'NE': '',\n",
              "                'POS_coarse': 'PART',\n",
              "                'POS_fine': 'TO',\n",
              "                'arc': 'aux',\n",
              "                'lemma': 'to',\n",
              "                'modifiers': [],\n",
              "                'word': 'to'},\n",
              "               {'NE': '',\n",
              "                'POS_coarse': 'NOUN',\n",
              "                'POS_fine': 'NN',\n",
              "                'arc': 'dobj',\n",
              "                'lemma': 'book',\n",
              "                'modifiers': [{'NE': '',\n",
              "                  'POS_coarse': 'DET',\n",
              "                  'POS_fine': 'DT',\n",
              "                  'arc': 'det',\n",
              "                  'lemma': 'a',\n",
              "                  'modifiers': [],\n",
              "                  'word': 'a'},\n",
              "                 {'NE': '',\n",
              "                  'POS_coarse': 'NOUN',\n",
              "                  'POS_fine': 'NN',\n",
              "                  'arc': 'compound',\n",
              "                  'lemma': 'specimen',\n",
              "                  'modifiers': [{'NE': '',\n",
              "                    'POS_coarse': 'NOUN',\n",
              "                    'POS_fine': 'NN',\n",
              "                    'arc': 'compound',\n",
              "                    'lemma': 'type',\n",
              "                    'modifiers': [],\n",
              "                    'word': 'type'}],\n",
              "                  'word': 'specimen'}],\n",
              "                'word': 'book'}],\n",
              "              'word': 'make'}],\n",
              "            'word': 'scrambled'}],\n",
              "          'word': 'took'}],\n",
              "        'word': 'the 1500s'}],\n",
              "      'word': 'since'}],\n",
              "    'word': 'text'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': '.',\n",
              "    'arc': 'punct',\n",
              "    'lemma': '.',\n",
              "    'modifiers': [],\n",
              "    'word': '.'}],\n",
              "  'word': 'been'},\n",
              " {'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VBN',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'survive',\n",
              "  'modifiers': [{'NE': '',\n",
              "    'POS_coarse': 'PRON',\n",
              "    'POS_fine': 'PRP',\n",
              "    'arc': 'nsubj',\n",
              "    'lemma': '-PRON-',\n",
              "    'modifiers': [],\n",
              "    'word': 'It'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'VERB',\n",
              "    'POS_fine': 'VBZ',\n",
              "    'arc': 'aux',\n",
              "    'lemma': 'have',\n",
              "    'modifiers': [],\n",
              "    'word': 'has'},\n",
              "   {'NE': 'DATE',\n",
              "    'POS_coarse': 'NOUN',\n",
              "    'POS_fine': 'NNS',\n",
              "    'arc': 'dobj',\n",
              "    'lemma': 'only five centuries',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'ADV',\n",
              "      'POS_fine': 'RB',\n",
              "      'arc': 'preconj',\n",
              "      'lemma': 'not',\n",
              "      'modifiers': [],\n",
              "      'word': 'not'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'PUNCT',\n",
              "      'POS_fine': ',',\n",
              "      'arc': 'punct',\n",
              "      'lemma': ',',\n",
              "      'modifiers': [],\n",
              "      'word': ','},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'CCONJ',\n",
              "      'POS_fine': 'CC',\n",
              "      'arc': 'cc',\n",
              "      'lemma': 'but',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'ADV',\n",
              "        'POS_fine': 'RB',\n",
              "        'arc': 'advmod',\n",
              "        'lemma': 'also',\n",
              "        'modifiers': [],\n",
              "        'word': 'also'}],\n",
              "      'word': 'but'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NN',\n",
              "      'arc': 'conj',\n",
              "      'lemma': 'leap',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'DET',\n",
              "        'POS_fine': 'DT',\n",
              "        'arc': 'det',\n",
              "        'lemma': 'the',\n",
              "        'modifiers': [],\n",
              "        'word': 'the'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'ADP',\n",
              "        'POS_fine': 'IN',\n",
              "        'arc': 'prep',\n",
              "        'lemma': 'into',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'NOUN',\n",
              "          'POS_fine': 'NN',\n",
              "          'arc': 'pobj',\n",
              "          'lemma': 'typesetting',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'ADJ',\n",
              "            'POS_fine': 'JJ',\n",
              "            'arc': 'amod',\n",
              "            'lemma': 'electronic',\n",
              "            'modifiers': [],\n",
              "            'word': 'electronic'}],\n",
              "          'word': 'typesetting'}],\n",
              "        'word': 'into'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'PUNCT',\n",
              "        'POS_fine': ',',\n",
              "        'arc': 'punct',\n",
              "        'lemma': ',',\n",
              "        'modifiers': [],\n",
              "        'word': ','},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'VERB',\n",
              "        'POS_fine': 'VBG',\n",
              "        'arc': 'acl',\n",
              "        'lemma': 'remain',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'ADJ',\n",
              "          'POS_fine': 'JJ',\n",
              "          'arc': 'acomp',\n",
              "          'lemma': 'unchanged',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'ADV',\n",
              "            'POS_fine': 'RB',\n",
              "            'arc': 'advmod',\n",
              "            'lemma': 'essentially',\n",
              "            'modifiers': [],\n",
              "            'word': 'essentially'}],\n",
              "          'word': 'unchanged'}],\n",
              "        'word': 'remaining'}],\n",
              "      'word': 'leap'}],\n",
              "    'word': 'only five centuries'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': '.',\n",
              "    'arc': 'punct',\n",
              "    'lemma': '.',\n",
              "    'modifiers': [],\n",
              "    'word': '.'}],\n",
              "  'word': 'survived'},\n",
              " {'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VBN',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'popularise',\n",
              "  'modifiers': [{'NE': '',\n",
              "    'POS_coarse': 'PRON',\n",
              "    'POS_fine': 'PRP',\n",
              "    'arc': 'nsubjpass',\n",
              "    'lemma': '-PRON-',\n",
              "    'modifiers': [],\n",
              "    'word': 'It'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'VERB',\n",
              "    'POS_fine': 'VBD',\n",
              "    'arc': 'auxpass',\n",
              "    'lemma': 'be',\n",
              "    'modifiers': [],\n",
              "    'word': 'was'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'ADP',\n",
              "    'POS_fine': 'IN',\n",
              "    'arc': 'prep',\n",
              "    'lemma': 'in',\n",
              "    'modifiers': [{'NE': 'DATE',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NNS',\n",
              "      'arc': 'pobj',\n",
              "      'lemma': 'the 1960s',\n",
              "      'modifiers': [],\n",
              "      'word': 'the 1960s'}],\n",
              "    'word': 'in'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'ADP',\n",
              "    'POS_fine': 'IN',\n",
              "    'arc': 'prep',\n",
              "    'lemma': 'with',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NN',\n",
              "      'arc': 'pobj',\n",
              "      'lemma': 'release',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'DET',\n",
              "        'POS_fine': 'DT',\n",
              "        'arc': 'det',\n",
              "        'lemma': 'the',\n",
              "        'modifiers': [],\n",
              "        'word': 'the'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'ADP',\n",
              "        'POS_fine': 'IN',\n",
              "        'arc': 'prep',\n",
              "        'lemma': 'of',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'NOUN',\n",
              "          'POS_fine': 'NNS',\n",
              "          'arc': 'pobj',\n",
              "          'lemma': 'sheet',\n",
              "          'modifiers': [{'NE': 'ORG',\n",
              "            'POS_coarse': 'PROPN',\n",
              "            'POS_fine': 'NNP',\n",
              "            'arc': 'compound',\n",
              "            'lemma': 'Letraset',\n",
              "            'modifiers': [],\n",
              "            'word': 'Letraset'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'VERB',\n",
              "            'POS_fine': 'VBG',\n",
              "            'arc': 'acl',\n",
              "            'lemma': 'contain',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'NOUN',\n",
              "              'POS_fine': 'NNS',\n",
              "              'arc': 'dobj',\n",
              "              'lemma': 'passage',\n",
              "              'modifiers': [{'NE': 'PERSON',\n",
              "                'POS_coarse': 'PROPN',\n",
              "                'POS_fine': 'NNP',\n",
              "                'arc': 'compound',\n",
              "                'lemma': 'Lorem Ipsum',\n",
              "                'modifiers': [],\n",
              "                'word': 'Lorem Ipsum'}],\n",
              "              'word': 'passages'}],\n",
              "            'word': 'containing'}],\n",
              "          'word': 'sheets'}],\n",
              "        'word': 'of'}],\n",
              "      'word': 'release'}],\n",
              "    'word': 'with'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': ',',\n",
              "    'arc': 'punct',\n",
              "    'lemma': ',',\n",
              "    'modifiers': [],\n",
              "    'word': ','},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'CCONJ',\n",
              "    'POS_fine': 'CC',\n",
              "    'arc': 'cc',\n",
              "    'lemma': 'and',\n",
              "    'modifiers': [],\n",
              "    'word': 'and'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'ADP',\n",
              "    'POS_fine': 'IN',\n",
              "    'arc': 'conj',\n",
              "    'lemma': 'with',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'ADV',\n",
              "      'POS_fine': 'RB',\n",
              "      'arc': 'advmod',\n",
              "      'lemma': 'recently',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'ADV',\n",
              "        'POS_fine': 'RBR',\n",
              "        'arc': 'advmod',\n",
              "        'lemma': 'more',\n",
              "        'modifiers': [],\n",
              "        'word': 'more'}],\n",
              "      'word': 'recently'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NN',\n",
              "      'arc': 'pobj',\n",
              "      'lemma': 'software',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NN',\n",
              "        'arc': 'compound',\n",
              "        'lemma': 'publishing',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'ADJ',\n",
              "          'POS_fine': 'JJ',\n",
              "          'arc': 'compound',\n",
              "          'lemma': 'desktop',\n",
              "          'modifiers': [],\n",
              "          'word': 'desktop'}],\n",
              "        'word': 'publishing'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'ADP',\n",
              "        'POS_fine': 'IN',\n",
              "        'arc': 'prep',\n",
              "        'lemma': 'like',\n",
              "        'modifiers': [{'NE': 'ORG',\n",
              "          'POS_coarse': 'PROPN',\n",
              "          'POS_fine': 'NNP',\n",
              "          'arc': 'pobj',\n",
              "          'lemma': 'Aldus PageMaker',\n",
              "          'modifiers': [],\n",
              "          'word': 'Aldus PageMaker'}],\n",
              "        'word': 'like'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'VERB',\n",
              "        'POS_fine': 'VBG',\n",
              "        'arc': 'prep',\n",
              "        'lemma': 'include',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'NOUN',\n",
              "          'POS_fine': 'NNS',\n",
              "          'arc': 'pobj',\n",
              "          'lemma': 'version',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'ADP',\n",
              "            'POS_fine': 'IN',\n",
              "            'arc': 'prep',\n",
              "            'lemma': 'of',\n",
              "            'modifiers': [{'NE': 'PERSON',\n",
              "              'POS_coarse': 'PROPN',\n",
              "              'POS_fine': 'NNP',\n",
              "              'arc': 'pobj',\n",
              "              'lemma': 'Lorem Ipsum',\n",
              "              'modifiers': [],\n",
              "              'word': 'Lorem Ipsum'}],\n",
              "            'word': 'of'}],\n",
              "          'word': 'versions'}],\n",
              "        'word': 'including'}],\n",
              "      'word': 'software'}],\n",
              "    'word': 'with'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': '.',\n",
              "    'arc': 'punct',\n",
              "    'lemma': '.',\n",
              "    'modifiers': [],\n",
              "    'word': '.'}],\n",
              "  'word': 'popularised'},\n",
              " {'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VBP',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'be',\n",
              "  'modifiers': [{'NE': '',\n",
              "    'POS_coarse': 'ADV',\n",
              "    'POS_fine': 'EX',\n",
              "    'arc': 'expl',\n",
              "    'lemma': 'there',\n",
              "    'modifiers': [],\n",
              "    'word': 'There'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'NOUN',\n",
              "    'POS_fine': 'NNS',\n",
              "    'arc': 'attr',\n",
              "    'lemma': 'variation',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'ADJ',\n",
              "      'POS_fine': 'JJ',\n",
              "      'arc': 'amod',\n",
              "      'lemma': 'many',\n",
              "      'modifiers': [],\n",
              "      'word': 'many'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'of',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NNS',\n",
              "        'arc': 'pobj',\n",
              "        'lemma': 'passage',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'ADP',\n",
              "          'POS_fine': 'IN',\n",
              "          'arc': 'prep',\n",
              "          'lemma': 'of',\n",
              "          'modifiers': [{'NE': 'PERSON',\n",
              "            'POS_coarse': 'PROPN',\n",
              "            'POS_fine': 'NNP',\n",
              "            'arc': 'pobj',\n",
              "            'lemma': 'Lorem Ipsum',\n",
              "            'modifiers': [],\n",
              "            'word': 'Lorem Ipsum'}],\n",
              "          'word': 'of'}],\n",
              "        'word': 'passages'}],\n",
              "      'word': 'of'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADJ',\n",
              "      'POS_fine': 'JJ',\n",
              "      'arc': 'amod',\n",
              "      'lemma': 'available',\n",
              "      'modifiers': [],\n",
              "      'word': 'available'}],\n",
              "    'word': 'variations'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': ',',\n",
              "    'arc': 'punct',\n",
              "    'lemma': ',',\n",
              "    'modifiers': [],\n",
              "    'word': ','},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'CCONJ',\n",
              "    'POS_fine': 'CC',\n",
              "    'arc': 'cc',\n",
              "    'lemma': 'but',\n",
              "    'modifiers': [],\n",
              "    'word': 'but'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'VERB',\n",
              "    'POS_fine': 'VBN',\n",
              "    'arc': 'conj',\n",
              "    'lemma': 'suffer',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NN',\n",
              "      'arc': 'nsubj',\n",
              "      'lemma': 'majority',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'DET',\n",
              "        'POS_fine': 'DT',\n",
              "        'arc': 'det',\n",
              "        'lemma': 'the',\n",
              "        'modifiers': [],\n",
              "        'word': 'the'}],\n",
              "      'word': 'majority'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'VERB',\n",
              "      'POS_fine': 'VBP',\n",
              "      'arc': 'aux',\n",
              "      'lemma': 'have',\n",
              "      'modifiers': [],\n",
              "      'word': 'have'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NN',\n",
              "      'arc': 'dobj',\n",
              "      'lemma': 'alteration',\n",
              "      'modifiers': [],\n",
              "      'word': 'alteration'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'in',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NN',\n",
              "        'arc': 'pobj',\n",
              "        'lemma': 'form',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'DET',\n",
              "          'POS_fine': 'DT',\n",
              "          'arc': 'det',\n",
              "          'lemma': 'some',\n",
              "          'modifiers': [],\n",
              "          'word': 'some'}],\n",
              "        'word': 'form'}],\n",
              "      'word': 'in'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'PUNCT',\n",
              "      'POS_fine': ',',\n",
              "      'arc': 'punct',\n",
              "      'lemma': ',',\n",
              "      'modifiers': [],\n",
              "      'word': ','},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'by',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NN',\n",
              "        'arc': 'pobj',\n",
              "        'lemma': 'humour',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'VERB',\n",
              "          'POS_fine': 'VBN',\n",
              "          'arc': 'amod',\n",
              "          'lemma': 'inject',\n",
              "          'modifiers': [],\n",
              "          'word': 'injected'}],\n",
              "        'word': 'humour'}],\n",
              "      'word': 'by'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'PUNCT',\n",
              "      'POS_fine': ',',\n",
              "      'arc': 'punct',\n",
              "      'lemma': ',',\n",
              "      'modifiers': [],\n",
              "      'word': ','},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'CCONJ',\n",
              "      'POS_fine': 'CC',\n",
              "      'arc': 'cc',\n",
              "      'lemma': 'or',\n",
              "      'modifiers': [],\n",
              "      'word': 'or'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'VERB',\n",
              "      'POS_fine': 'VBN',\n",
              "      'arc': 'conj',\n",
              "      'lemma': 'randomise',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NNS',\n",
              "        'arc': 'dobj',\n",
              "        'lemma': 'word',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'VERB',\n",
              "          'POS_fine': 'VB',\n",
              "          'arc': 'relcl',\n",
              "          'lemma': 'look',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'ADJ',\n",
              "            'POS_fine': 'WDT',\n",
              "            'arc': 'nsubj',\n",
              "            'lemma': 'which',\n",
              "            'modifiers': [],\n",
              "            'word': 'which'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'VERB',\n",
              "            'POS_fine': 'VBP',\n",
              "            'arc': 'aux',\n",
              "            'lemma': 'do',\n",
              "            'modifiers': [],\n",
              "            'word': 'do'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'ADV',\n",
              "            'POS_fine': 'RB',\n",
              "            'arc': 'neg',\n",
              "            'lemma': \"n't\",\n",
              "            'modifiers': [],\n",
              "            'word': \"n't\"},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'ADJ',\n",
              "            'POS_fine': 'JJ',\n",
              "            'arc': 'acomp',\n",
              "            'lemma': 'believable',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'ADV',\n",
              "              'POS_fine': 'RB',\n",
              "              'arc': 'advmod',\n",
              "              'lemma': 'slightly',\n",
              "              'modifiers': [{'NE': '',\n",
              "                'POS_coarse': 'ADV',\n",
              "                'POS_fine': 'RB',\n",
              "                'arc': 'advmod',\n",
              "                'lemma': 'even',\n",
              "                'modifiers': [],\n",
              "                'word': 'even'}],\n",
              "              'word': 'slightly'}],\n",
              "            'word': 'believable'}],\n",
              "          'word': 'look'}],\n",
              "        'word': 'words'}],\n",
              "      'word': 'randomised'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'PUNCT',\n",
              "      'POS_fine': '.',\n",
              "      'arc': 'punct',\n",
              "      'lemma': '.',\n",
              "      'modifiers': [],\n",
              "      'word': '.'}],\n",
              "    'word': 'suffered'}],\n",
              "  'word': 'are'},\n",
              " {'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VBP',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'need',\n",
              "  'modifiers': [{'NE': '',\n",
              "    'POS_coarse': 'VERB',\n",
              "    'POS_fine': 'VBG',\n",
              "    'arc': 'advcl',\n",
              "    'lemma': 'go',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'mark',\n",
              "      'lemma': 'if',\n",
              "      'modifiers': [],\n",
              "      'word': 'If'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'PRON',\n",
              "      'POS_fine': 'PRP',\n",
              "      'arc': 'nsubj',\n",
              "      'lemma': '-PRON-',\n",
              "      'modifiers': [],\n",
              "      'word': 'you'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'VERB',\n",
              "      'POS_fine': 'VBP',\n",
              "      'arc': 'aux',\n",
              "      'lemma': 'be',\n",
              "      'modifiers': [],\n",
              "      'word': 'are'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'VERB',\n",
              "      'POS_fine': 'VB',\n",
              "      'arc': 'xcomp',\n",
              "      'lemma': 'use',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'PART',\n",
              "        'POS_fine': 'TO',\n",
              "        'arc': 'aux',\n",
              "        'lemma': 'to',\n",
              "        'modifiers': [],\n",
              "        'word': 'to'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NN',\n",
              "        'arc': 'dobj',\n",
              "        'lemma': 'passage',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'DET',\n",
              "          'POS_fine': 'DT',\n",
              "          'arc': 'det',\n",
              "          'lemma': 'a',\n",
              "          'modifiers': [],\n",
              "          'word': 'a'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'ADP',\n",
              "          'POS_fine': 'IN',\n",
              "          'arc': 'prep',\n",
              "          'lemma': 'of',\n",
              "          'modifiers': [{'NE': 'PERSON',\n",
              "            'POS_coarse': 'PROPN',\n",
              "            'POS_fine': 'NNP',\n",
              "            'arc': 'pobj',\n",
              "            'lemma': 'Lorem Ipsum',\n",
              "            'modifiers': [],\n",
              "            'word': 'Lorem Ipsum'}],\n",
              "          'word': 'of'}],\n",
              "        'word': 'passage'}],\n",
              "      'word': 'use'}],\n",
              "    'word': 'going'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': ',',\n",
              "    'arc': 'punct',\n",
              "    'lemma': ',',\n",
              "    'modifiers': [],\n",
              "    'word': ','},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PRON',\n",
              "    'POS_fine': 'PRP',\n",
              "    'arc': 'nsubj',\n",
              "    'lemma': '-PRON-',\n",
              "    'modifiers': [],\n",
              "    'word': 'you'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'VERB',\n",
              "    'POS_fine': 'VB',\n",
              "    'arc': 'xcomp',\n",
              "    'lemma': 'be',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'PART',\n",
              "      'POS_fine': 'TO',\n",
              "      'arc': 'aux',\n",
              "      'lemma': 'to',\n",
              "      'modifiers': [],\n",
              "      'word': 'to'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADJ',\n",
              "      'POS_fine': 'JJ',\n",
              "      'arc': 'acomp',\n",
              "      'lemma': 'sure',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'VERB',\n",
              "        'POS_fine': 'VBZ',\n",
              "        'arc': 'ccomp',\n",
              "        'lemma': 'be',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'ADV',\n",
              "          'POS_fine': 'EX',\n",
              "          'arc': 'expl',\n",
              "          'lemma': 'there',\n",
              "          'modifiers': [],\n",
              "          'word': 'there'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'ADV',\n",
              "          'POS_fine': 'RB',\n",
              "          'arc': 'neg',\n",
              "          'lemma': \"n't\",\n",
              "          'modifiers': [],\n",
              "          'word': \"n't\"},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'NOUN',\n",
              "          'POS_fine': 'NN',\n",
              "          'arc': 'attr',\n",
              "          'lemma': 'anything',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'VERB',\n",
              "            'POS_fine': 'VBG',\n",
              "            'arc': 'amod',\n",
              "            'lemma': 'embarrass',\n",
              "            'modifiers': [],\n",
              "            'word': 'embarrassing'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'VERB',\n",
              "            'POS_fine': 'VBN',\n",
              "            'arc': 'acl',\n",
              "            'lemma': 'hide',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'ADP',\n",
              "              'POS_fine': 'IN',\n",
              "              'arc': 'prep',\n",
              "              'lemma': 'in',\n",
              "              'modifiers': [{'NE': '',\n",
              "                'POS_coarse': 'NOUN',\n",
              "                'POS_fine': 'NN',\n",
              "                'arc': 'pobj',\n",
              "                'lemma': 'middle',\n",
              "                'modifiers': [{'NE': '',\n",
              "                  'POS_coarse': 'DET',\n",
              "                  'POS_fine': 'DT',\n",
              "                  'arc': 'det',\n",
              "                  'lemma': 'the',\n",
              "                  'modifiers': [],\n",
              "                  'word': 'the'},\n",
              "                 {'NE': '',\n",
              "                  'POS_coarse': 'ADP',\n",
              "                  'POS_fine': 'IN',\n",
              "                  'arc': 'prep',\n",
              "                  'lemma': 'of',\n",
              "                  'modifiers': [{'NE': '',\n",
              "                    'POS_coarse': 'NOUN',\n",
              "                    'POS_fine': 'NN',\n",
              "                    'arc': 'pobj',\n",
              "                    'lemma': 'text',\n",
              "                    'modifiers': [],\n",
              "                    'word': 'text'}],\n",
              "                  'word': 'of'}],\n",
              "                'word': 'middle'}],\n",
              "              'word': 'in'}],\n",
              "            'word': 'hidden'}],\n",
              "          'word': 'anything'}],\n",
              "        'word': 'is'}],\n",
              "      'word': 'sure'}],\n",
              "    'word': 'be'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': '.',\n",
              "    'arc': 'punct',\n",
              "    'lemma': '.',\n",
              "    'modifiers': [],\n",
              "    'word': '.'}],\n",
              "  'word': 'need'},\n",
              " {'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VB',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'tend',\n",
              "  'modifiers': [{'NE': '',\n",
              "    'POS_coarse': 'NOUN',\n",
              "    'POS_fine': 'NNS',\n",
              "    'arc': 'nsubj',\n",
              "    'lemma': 'generator',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'PROPN',\n",
              "      'POS_fine': 'NNP',\n",
              "      'arc': 'nsubj',\n",
              "      'lemma': 'ipsum',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'ADJ',\n",
              "        'POS_fine': 'PDT',\n",
              "        'arc': 'predet',\n",
              "        'lemma': 'all',\n",
              "        'modifiers': [],\n",
              "        'word': 'All'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'DET',\n",
              "        'POS_fine': 'DT',\n",
              "        'arc': 'det',\n",
              "        'lemma': 'the',\n",
              "        'modifiers': [],\n",
              "        'word': 'the'},\n",
              "       {'NE': '',\n",
              "        'POS_coarse': 'PROPN',\n",
              "        'POS_fine': 'NNP',\n",
              "        'arc': 'compound',\n",
              "        'lemma': 'lorem',\n",
              "        'modifiers': [],\n",
              "        'word': 'Lorem'}],\n",
              "      'word': 'Ipsum'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'on',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NN',\n",
              "        'arc': 'pobj',\n",
              "        'lemma': 'internet',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'DET',\n",
              "          'POS_fine': 'DT',\n",
              "          'arc': 'det',\n",
              "          'lemma': 'the',\n",
              "          'modifiers': [],\n",
              "          'word': 'the'}],\n",
              "        'word': 'Internet'}],\n",
              "      'word': 'on'}],\n",
              "    'word': 'generators'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'VERB',\n",
              "    'POS_fine': 'VB',\n",
              "    'arc': 'xcomp',\n",
              "    'lemma': 'repeat',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'PART',\n",
              "      'POS_fine': 'TO',\n",
              "      'arc': 'aux',\n",
              "      'lemma': 'to',\n",
              "      'modifiers': [],\n",
              "      'word': 'to'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'NOUN',\n",
              "      'POS_fine': 'NNS',\n",
              "      'arc': 'dobj',\n",
              "      'lemma': 'chunk',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'ADJ',\n",
              "        'POS_fine': 'JJ',\n",
              "        'arc': 'amod',\n",
              "        'lemma': 'predefined',\n",
              "        'modifiers': [],\n",
              "        'word': 'predefined'}],\n",
              "      'word': 'chunks'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'as',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'ADJ',\n",
              "        'POS_fine': 'JJ',\n",
              "        'arc': 'amod',\n",
              "        'lemma': 'necessary',\n",
              "        'modifiers': [],\n",
              "        'word': 'necessary'}],\n",
              "      'word': 'as'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'PUNCT',\n",
              "      'POS_fine': ',',\n",
              "      'arc': 'punct',\n",
              "      'lemma': ',',\n",
              "      'modifiers': [],\n",
              "      'word': ','},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'VERB',\n",
              "      'POS_fine': 'VBG',\n",
              "      'arc': 'advcl',\n",
              "      'lemma': 'make',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NN',\n",
              "        'arc': 'ccomp',\n",
              "        'lemma': 'generator',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'DET',\n",
              "          'POS_fine': 'DT',\n",
              "          'arc': 'nsubj',\n",
              "          'lemma': 'this',\n",
              "          'modifiers': [],\n",
              "          'word': 'this'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'DET',\n",
              "          'POS_fine': 'DT',\n",
              "          'arc': 'det',\n",
              "          'lemma': 'the',\n",
              "          'modifiers': [],\n",
              "          'word': 'the'},\n",
              "         {'NE': 'ORDINAL',\n",
              "          'POS_coarse': 'ADJ',\n",
              "          'POS_fine': 'JJ',\n",
              "          'arc': 'amod',\n",
              "          'lemma': 'first',\n",
              "          'modifiers': [],\n",
              "          'word': 'first'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'ADJ',\n",
              "          'POS_fine': 'JJ',\n",
              "          'arc': 'amod',\n",
              "          'lemma': 'true',\n",
              "          'modifiers': [],\n",
              "          'word': 'true'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'ADP',\n",
              "          'POS_fine': 'IN',\n",
              "          'arc': 'prep',\n",
              "          'lemma': 'on',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'NOUN',\n",
              "            'POS_fine': 'NN',\n",
              "            'arc': 'pobj',\n",
              "            'lemma': 'internet',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'DET',\n",
              "              'POS_fine': 'DT',\n",
              "              'arc': 'det',\n",
              "              'lemma': 'the',\n",
              "              'modifiers': [],\n",
              "              'word': 'the'}],\n",
              "            'word': 'Internet'}],\n",
              "          'word': 'on'}],\n",
              "        'word': 'generator'}],\n",
              "      'word': 'making'}],\n",
              "    'word': 'repeat'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': '.',\n",
              "    'arc': 'punct',\n",
              "    'lemma': '.',\n",
              "    'modifiers': [],\n",
              "    'word': '.'}],\n",
              "  'word': 'tend'},\n",
              " {'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VBZ',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'use',\n",
              "  'modifiers': [{'NE': '',\n",
              "    'POS_coarse': 'PRON',\n",
              "    'POS_fine': 'PRP',\n",
              "    'arc': 'nsubj',\n",
              "    'lemma': '-PRON-',\n",
              "    'modifiers': [],\n",
              "    'word': 'It'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'NOUN',\n",
              "    'POS_fine': 'NN',\n",
              "    'arc': 'dobj',\n",
              "    'lemma': 'dictionary',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'DET',\n",
              "      'POS_fine': 'DT',\n",
              "      'arc': 'det',\n",
              "      'lemma': 'a',\n",
              "      'modifiers': [],\n",
              "      'word': 'a'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'of',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NNS',\n",
              "        'arc': 'pobj',\n",
              "        'lemma': 'word',\n",
              "        'modifiers': [{'NE': 'CARDINAL',\n",
              "          'POS_coarse': 'NUM',\n",
              "          'POS_fine': 'CD',\n",
              "          'arc': 'nummod',\n",
              "          'lemma': '200',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'ADP',\n",
              "            'POS_fine': 'IN',\n",
              "            'arc': 'quantmod',\n",
              "            'lemma': 'over',\n",
              "            'modifiers': [],\n",
              "            'word': 'over'}],\n",
              "          'word': '200'},\n",
              "         {'NE': 'NORP',\n",
              "          'POS_coarse': 'ADJ',\n",
              "          'POS_fine': 'JJ',\n",
              "          'arc': 'amod',\n",
              "          'lemma': 'Latin',\n",
              "          'modifiers': [],\n",
              "          'word': 'Latin'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'PUNCT',\n",
              "          'POS_fine': ',',\n",
              "          'arc': 'punct',\n",
              "          'lemma': ',',\n",
              "          'modifiers': [],\n",
              "          'word': ','},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'VERB',\n",
              "          'POS_fine': 'VBN',\n",
              "          'arc': 'acl',\n",
              "          'lemma': 'combine',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'ADP',\n",
              "            'POS_fine': 'IN',\n",
              "            'arc': 'prep',\n",
              "            'lemma': 'with',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'NOUN',\n",
              "              'POS_fine': 'NN',\n",
              "              'arc': 'pobj',\n",
              "              'lemma': 'handful',\n",
              "              'modifiers': [{'NE': '',\n",
              "                'POS_coarse': 'DET',\n",
              "                'POS_fine': 'DT',\n",
              "                'arc': 'det',\n",
              "                'lemma': 'a',\n",
              "                'modifiers': [],\n",
              "                'word': 'a'},\n",
              "               {'NE': '',\n",
              "                'POS_coarse': 'ADP',\n",
              "                'POS_fine': 'IN',\n",
              "                'arc': 'prep',\n",
              "                'lemma': 'of',\n",
              "                'modifiers': [{'NE': '',\n",
              "                  'POS_coarse': 'NOUN',\n",
              "                  'POS_fine': 'NNS',\n",
              "                  'arc': 'pobj',\n",
              "                  'lemma': 'structure',\n",
              "                  'modifiers': [{'NE': '',\n",
              "                    'POS_coarse': 'NOUN',\n",
              "                    'POS_fine': 'NN',\n",
              "                    'arc': 'compound',\n",
              "                    'lemma': 'model',\n",
              "                    'modifiers': [],\n",
              "                    'word': 'model'},\n",
              "                   {'NE': '',\n",
              "                    'POS_coarse': 'NOUN',\n",
              "                    'POS_fine': 'NN',\n",
              "                    'arc': 'compound',\n",
              "                    'lemma': 'sentence',\n",
              "                    'modifiers': [],\n",
              "                    'word': 'sentence'}],\n",
              "                  'word': 'structures'}],\n",
              "                'word': 'of'}],\n",
              "              'word': 'handful'}],\n",
              "            'word': 'with'}],\n",
              "          'word': 'combined'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'PUNCT',\n",
              "          'POS_fine': ',',\n",
              "          'arc': 'punct',\n",
              "          'lemma': ',',\n",
              "          'modifiers': [],\n",
              "          'word': ','}],\n",
              "        'word': 'words'}],\n",
              "      'word': 'of'}],\n",
              "    'word': 'dictionary'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'VERB',\n",
              "    'POS_fine': 'VB',\n",
              "    'arc': 'xcomp',\n",
              "    'lemma': 'generate',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'PART',\n",
              "      'POS_fine': 'TO',\n",
              "      'arc': 'aux',\n",
              "      'lemma': 'to',\n",
              "      'modifiers': [],\n",
              "      'word': 'to'},\n",
              "     {'NE': 'PERSON',\n",
              "      'POS_coarse': 'PROPN',\n",
              "      'POS_fine': 'NNP',\n",
              "      'arc': 'dobj',\n",
              "      'lemma': 'Lorem Ipsum',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'VERB',\n",
              "        'POS_fine': 'VBZ',\n",
              "        'arc': 'relcl',\n",
              "        'lemma': 'look',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'ADJ',\n",
              "          'POS_fine': 'WDT',\n",
              "          'arc': 'nsubj',\n",
              "          'lemma': 'which',\n",
              "          'modifiers': [],\n",
              "          'word': 'which'},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'ADJ',\n",
              "          'POS_fine': 'JJ',\n",
              "          'arc': 'acomp',\n",
              "          'lemma': 'reasonable',\n",
              "          'modifiers': [],\n",
              "          'word': 'reasonable'}],\n",
              "        'word': 'looks'}],\n",
              "      'word': 'Lorem Ipsum'}],\n",
              "    'word': 'generate'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': '.',\n",
              "    'arc': 'punct',\n",
              "    'lemma': '.',\n",
              "    'modifiers': [],\n",
              "    'word': '.'}],\n",
              "  'word': 'uses'},\n",
              " {'NE': '',\n",
              "  'POS_coarse': 'VERB',\n",
              "  'POS_fine': 'VBZ',\n",
              "  'arc': 'ROOT',\n",
              "  'lemma': 'be',\n",
              "  'modifiers': [{'NE': 'PERSON',\n",
              "    'POS_coarse': 'PROPN',\n",
              "    'POS_fine': 'NNP',\n",
              "    'arc': 'nsubj',\n",
              "    'lemma': 'Lorem Ipsum',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'DET',\n",
              "      'POS_fine': 'DT',\n",
              "      'arc': 'det',\n",
              "      'lemma': 'the',\n",
              "      'modifiers': [],\n",
              "      'word': 'The'},\n",
              "     {'NE': '',\n",
              "      'POS_coarse': 'VERB',\n",
              "      'POS_fine': 'VBN',\n",
              "      'arc': 'amod',\n",
              "      'lemma': 'generate',\n",
              "      'modifiers': [],\n",
              "      'word': 'generated'}],\n",
              "    'word': 'Lorem Ipsum'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'ADV',\n",
              "    'POS_fine': 'RB',\n",
              "    'arc': 'advmod',\n",
              "    'lemma': 'therefore',\n",
              "    'modifiers': [],\n",
              "    'word': 'therefore'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'ADV',\n",
              "    'POS_fine': 'RB',\n",
              "    'arc': 'advmod',\n",
              "    'lemma': 'always',\n",
              "    'modifiers': [],\n",
              "    'word': 'always'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'ADJ',\n",
              "    'POS_fine': 'JJ',\n",
              "    'arc': 'acomp',\n",
              "    'lemma': 'free',\n",
              "    'modifiers': [{'NE': '',\n",
              "      'POS_coarse': 'ADP',\n",
              "      'POS_fine': 'IN',\n",
              "      'arc': 'prep',\n",
              "      'lemma': 'from',\n",
              "      'modifiers': [{'NE': '',\n",
              "        'POS_coarse': 'NOUN',\n",
              "        'POS_fine': 'NN',\n",
              "        'arc': 'pobj',\n",
              "        'lemma': 'repetition',\n",
              "        'modifiers': [{'NE': '',\n",
              "          'POS_coarse': 'PUNCT',\n",
              "          'POS_fine': ',',\n",
              "          'arc': 'punct',\n",
              "          'lemma': ',',\n",
              "          'modifiers': [],\n",
              "          'word': ','},\n",
              "         {'NE': '',\n",
              "          'POS_coarse': 'NOUN',\n",
              "          'POS_fine': 'NN',\n",
              "          'arc': 'conj',\n",
              "          'lemma': 'humour',\n",
              "          'modifiers': [{'NE': '',\n",
              "            'POS_coarse': 'VERB',\n",
              "            'POS_fine': 'VBD',\n",
              "            'arc': 'amod',\n",
              "            'lemma': 'inject',\n",
              "            'modifiers': [],\n",
              "            'word': 'injected'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'PUNCT',\n",
              "            'POS_fine': ',',\n",
              "            'arc': 'punct',\n",
              "            'lemma': ',',\n",
              "            'modifiers': [],\n",
              "            'word': ','},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'CCONJ',\n",
              "            'POS_fine': 'CC',\n",
              "            'arc': 'cc',\n",
              "            'lemma': 'or',\n",
              "            'modifiers': [],\n",
              "            'word': 'or'},\n",
              "           {'NE': '',\n",
              "            'POS_coarse': 'X',\n",
              "            'POS_fine': 'FW',\n",
              "            'arc': 'conj',\n",
              "            'lemma': 'etc',\n",
              "            'modifiers': [{'NE': '',\n",
              "              'POS_coarse': 'NOUN',\n",
              "              'POS_fine': 'NNS',\n",
              "              'arc': 'compound',\n",
              "              'lemma': 'word',\n",
              "              'modifiers': [{'NE': '',\n",
              "                'POS_coarse': 'ADJ',\n",
              "                'POS_fine': 'JJ',\n",
              "                'arc': 'amod',\n",
              "                'lemma': 'characteristic',\n",
              "                'modifiers': [{'NE': '',\n",
              "                  'POS_coarse': 'PUNCT',\n",
              "                  'POS_fine': 'HYPH',\n",
              "                  'arc': 'punct',\n",
              "                  'lemma': '-',\n",
              "                  'modifiers': [{'NE': '',\n",
              "                    'POS_coarse': 'ADJ',\n",
              "                    'POS_fine': 'AFX',\n",
              "                    'arc': 'nmod',\n",
              "                    'lemma': 'non',\n",
              "                    'modifiers': [],\n",
              "                    'word': 'non'}],\n",
              "                  'word': '-'}],\n",
              "                'word': 'characteristic'}],\n",
              "              'word': 'words'}],\n",
              "            'word': 'etc'}],\n",
              "          'word': 'humour'}],\n",
              "        'word': 'repetition'}],\n",
              "      'word': 'from'}],\n",
              "    'word': 'free'},\n",
              "   {'NE': '',\n",
              "    'POS_coarse': 'PUNCT',\n",
              "    'POS_fine': '.',\n",
              "    'arc': 'punct',\n",
              "    'lemma': '.',\n",
              "    'modifiers': [],\n",
              "    'word': '.'}],\n",
              "  'word': 'is'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zy5HjD_APxc"
      },
      "source": [
        "# check if a word is a stopword\n",
        "lemmas = [token.lemma_ for token in doc if not token.is_stop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zr8qETHAPxf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2675
        },
        "outputId": "e0ea1e8a-f110-4240-e5ea-92e5b5480967"
      },
      "source": [
        "lemmas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lorem',\n",
              " 'ipsum',\n",
              " 'simply',\n",
              " 'dummy',\n",
              " 'text',\n",
              " 'printing',\n",
              " 'typesetting',\n",
              " 'industry',\n",
              " '.',\n",
              " 'lorem',\n",
              " 'ipsum',\n",
              " 'industry',\n",
              " \"'s\",\n",
              " 'standard',\n",
              " 'dummy',\n",
              " 'text',\n",
              " '1500',\n",
              " ',',\n",
              " 'unknown',\n",
              " 'printer',\n",
              " 'take',\n",
              " 'galley',\n",
              " 'type',\n",
              " 'scramble',\n",
              " 'type',\n",
              " 'specimen',\n",
              " 'book',\n",
              " '.',\n",
              " '-PRON-',\n",
              " 'survive',\n",
              " 'century',\n",
              " ',',\n",
              " 'leap',\n",
              " 'electronic',\n",
              " 'typesetting',\n",
              " ',',\n",
              " 'remain',\n",
              " 'essentially',\n",
              " 'unchanged',\n",
              " '.',\n",
              " '-PRON-',\n",
              " 'popularise',\n",
              " '1960',\n",
              " 'release',\n",
              " 'letraset',\n",
              " 'sheet',\n",
              " 'contain',\n",
              " 'lorem',\n",
              " 'ipsum',\n",
              " 'passage',\n",
              " ',',\n",
              " 'recently',\n",
              " 'desktop',\n",
              " 'publishing',\n",
              " 'software',\n",
              " 'like',\n",
              " 'aldus',\n",
              " 'pagemaker',\n",
              " 'include',\n",
              " 'version',\n",
              " 'lorem',\n",
              " 'ipsum',\n",
              " '.',\n",
              " 'there',\n",
              " 'variation',\n",
              " 'passage',\n",
              " 'lorem',\n",
              " 'ipsum',\n",
              " 'available',\n",
              " ',',\n",
              " 'majority',\n",
              " 'suffer',\n",
              " 'alteration',\n",
              " 'form',\n",
              " ',',\n",
              " 'inject',\n",
              " 'humour',\n",
              " ',',\n",
              " 'randomise',\n",
              " 'word',\n",
              " 'not',\n",
              " 'look',\n",
              " 'slightly',\n",
              " 'believable',\n",
              " '.',\n",
              " 'if',\n",
              " 'go',\n",
              " 'use',\n",
              " 'passage',\n",
              " 'lorem',\n",
              " 'ipsum',\n",
              " ',',\n",
              " 'need',\n",
              " 'sure',\n",
              " 'not',\n",
              " 'embarrass',\n",
              " 'hide',\n",
              " 'middle',\n",
              " 'text',\n",
              " '.',\n",
              " 'all',\n",
              " 'lorem',\n",
              " 'ipsum',\n",
              " 'generator',\n",
              " 'internet',\n",
              " 'tend',\n",
              " 'repeat',\n",
              " 'predefined',\n",
              " 'chunk',\n",
              " 'necessary',\n",
              " ',',\n",
              " 'make',\n",
              " 'true',\n",
              " 'generator',\n",
              " 'internet',\n",
              " '.',\n",
              " '-PRON-',\n",
              " 'use',\n",
              " 'dictionary',\n",
              " '200',\n",
              " 'latin',\n",
              " 'word',\n",
              " ',',\n",
              " 'combine',\n",
              " 'handful',\n",
              " 'model',\n",
              " 'sentence',\n",
              " 'structure',\n",
              " ',',\n",
              " 'generate',\n",
              " 'lorem',\n",
              " 'ipsum',\n",
              " 'look',\n",
              " 'reasonable',\n",
              " '.',\n",
              " 'the',\n",
              " 'generate',\n",
              " 'lorem',\n",
              " 'ipsum',\n",
              " 'free',\n",
              " 'repetition',\n",
              " ',',\n",
              " 'inject',\n",
              " 'humour',\n",
              " ',',\n",
              " 'non',\n",
              " '-',\n",
              " 'characteristic',\n",
              " 'word',\n",
              " 'etc',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3VYN7_XAPxj"
      },
      "source": [
        "def normalize2(comment, lowercase=True, remove_stopwords=True):\n",
        "    if lowercase:\n",
        "        comment = comment.lower()\n",
        "    comment = nlp(comment)\n",
        "    lemmatized = list()\n",
        "    for word in comment:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
        "                lemmatized.append(lemma)\n",
        "    return \" \".join(lemmatized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmbMpxqOAPxm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "cd8b3959-81af-40e3-e06d-979d6fe66121"
      },
      "source": [
        "normalize2(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"lorem ipsum simply dummy text printing typesetting industry . lorem ipsum industry 's standard dummy text ever since 1500 , unknown printer take galley type scramble -PRON- make type specimen book . -PRON- survive five century , also leap electronic typesetting , remain essentially unchanged . -PRON- popularise 1960 release letraset sheet contain lorem ipsum passage , recently desktop publishing software like aldus pagemaker include version lorem ipsum.there many variation passage lorem ipsum available , majority suffer alteration form , inject humour , randomise word look even slightly believable . -PRON- go use passage lorem ipsum , -PRON- need sure anything embarrass hide middle text . lorem ipsum generator internet tend repeat predefined chunk necessary , make first true generator internet . -PRON- use dictionary 200 latin word , combine handful model sentence structure , generate lorem ipsum look reasonable . generate lorem ipsum therefore always free repetition , inject humour , non - characteristic word etc .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9tZRXUZAPxp"
      },
      "source": [
        "# Process whole documents\n",
        "text3 = (u\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        u\"Google in 2007, few people outside of the company took him \"\n",
        "        u\"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        u\"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        u\"worth talking to,” said Thrun, now the co-founder and CEO of \"\n",
        "        u\"online higher education startup Udacity, in an interview with \"\n",
        "        u\"Recode earlier this week.\")\n",
        "doc2 = nlp(text3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6skn8qHbiDW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "f96a9634-18d1-478d-c1fe-f4be1f4f388f"
      },
      "source": [
        "# Find named entities, phrases and concepts\n",
        "for entity in doc2.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sebastian Thrun PERSON\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Thrun PERSON\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69VStolKbiXL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f5e7b7a-b7c6-4013-933a-92c8ade44c5f"
      },
      "source": [
        "# Determine semantic similarities\n",
        "doc1 = nlp(u\"my fries were super gross\")\n",
        "doc2 = nlp(u\"such disgusting fries\")\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(doc1.text, doc2.text, similarity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my fries were super gross such disgusting fries 0.7139701576579747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eis87ZknAPxz"
      },
      "source": [
        "def normalize(s):\n",
        "    \"\"\"\n",
        "    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
        "        \"\"\"\n",
        "    s = s.lower()\n",
        "    # Replace ips\n",
        "    s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n",
        "    # Isolate punctuation\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Replace numbers and symbols with language\n",
        "    s = s.replace('&', ' and ')\n",
        "    s = s.replace('@', ' at ')\n",
        "    s = s.replace('0', ' zero ')\n",
        "    s = s.replace('1', ' one ')\n",
        "    s = s.replace('2', ' two ')\n",
        "    s = s.replace('3', ' three ')\n",
        "    s = s.replace('4', ' four ')\n",
        "    s = s.replace('5', ' five ')\n",
        "    s = s.replace('6', ' six ')\n",
        "    s = s.replace('7', ' seven ')\n",
        "    s = s.replace('8', ' eight ')\n",
        "    s = s.replace('9', ' nine ')\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhKk3fdHAPx1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a0da559c-5db9-4c9a-bdcd-6d06b48ec9bd"
      },
      "source": [
        "normalize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"lorem ipsum is simply dummy text of the printing and typesetting industry .  lorem ipsum has been the industry ' s standard dummy text ever since the  one  five  zero  zero s ,  when an unknown printer took a galley of type and scrambled it to make a type specimen book .  it has survived not only five centuries ,  but also the leap into electronic typesetting ,  remaining essentially unchanged .  it was popularised in the  one  nine  six  zero s with the release of letraset sheets containing lorem ipsum passages ,  and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum . there are many variations of passages of lorem ipsum available ,  but the majority have suffered alteration in some form ,  by injected humour ,  or randomised words which don ' t look even slightly believable .  if you are going to use a passage of lorem ipsum ,  you need to be sure there isn ' t anything embarrassing hidden in the middle of text .  all the lorem ipsum generators on the internet tend to repeat predefined chunks as necessary ,  making this the first true generator on the internet .  it uses a dictionary of over  two  zero  zero  latin words ,  combined with a handful of model sentence structures ,  to generate lorem ipsum which looks reasonable .  the generated lorem ipsum is therefore always free from repetition ,  injected humour ,  or non - characteristic words etc . \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiaCXlB1APx5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4223
        },
        "outputId": "4baeeb5e-ee55-4530-e03b-64ec4bb855fa"
      },
      "source": [
        "for token in doc:\n",
        "    print([token.text, token.pos_, token.dep_])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'nsubj']\n",
            "['is', 'VERB', 'ROOT']\n",
            "['simply', 'ADV', 'advmod']\n",
            "['dummy', 'ADJ', 'amod']\n",
            "['text', 'NOUN', 'attr']\n",
            "['of', 'ADP', 'prep']\n",
            "['the', 'DET', 'det']\n",
            "['printing', 'NOUN', 'amod']\n",
            "['and', 'CCONJ', 'cc']\n",
            "['typesetting', 'NOUN', 'conj']\n",
            "['industry', 'NOUN', 'pobj']\n",
            "['.', 'PUNCT', 'punct']\n",
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'nsubj']\n",
            "['has', 'VERB', 'aux']\n",
            "['been', 'VERB', 'ROOT']\n",
            "['the', 'DET', 'det']\n",
            "['industry', 'NOUN', 'poss']\n",
            "[\"'s\", 'PART', 'case']\n",
            "['standard', 'ADJ', 'amod']\n",
            "['dummy', 'NOUN', 'amod']\n",
            "['text', 'NOUN', 'attr']\n",
            "['ever', 'ADV', 'advmod']\n",
            "['since', 'ADP', 'prep']\n",
            "['the', 'DET', 'det']\n",
            "['1500s', 'NOUN', 'pobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['when', 'ADV', 'advmod']\n",
            "['an', 'DET', 'det']\n",
            "['unknown', 'ADJ', 'amod']\n",
            "['printer', 'NOUN', 'nsubj']\n",
            "['took', 'VERB', 'relcl']\n",
            "['a', 'DET', 'det']\n",
            "['galley', 'NOUN', 'dobj']\n",
            "['of', 'ADP', 'prep']\n",
            "['type', 'NOUN', 'pobj']\n",
            "['and', 'CCONJ', 'cc']\n",
            "['scrambled', 'VERB', 'conj']\n",
            "['it', 'PRON', 'dobj']\n",
            "['to', 'PART', 'aux']\n",
            "['make', 'VERB', 'advcl']\n",
            "['a', 'DET', 'det']\n",
            "['type', 'NOUN', 'compound']\n",
            "['specimen', 'NOUN', 'compound']\n",
            "['book', 'NOUN', 'dobj']\n",
            "['.', 'PUNCT', 'punct']\n",
            "['It', 'PRON', 'nsubj']\n",
            "['has', 'VERB', 'aux']\n",
            "['survived', 'VERB', 'ROOT']\n",
            "['not', 'ADV', 'preconj']\n",
            "['only', 'ADV', 'advmod']\n",
            "['five', 'NUM', 'nummod']\n",
            "['centuries', 'NOUN', 'dobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['but', 'CCONJ', 'cc']\n",
            "['also', 'ADV', 'advmod']\n",
            "['the', 'DET', 'det']\n",
            "['leap', 'NOUN', 'conj']\n",
            "['into', 'ADP', 'prep']\n",
            "['electronic', 'ADJ', 'amod']\n",
            "['typesetting', 'NOUN', 'pobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['remaining', 'VERB', 'acl']\n",
            "['essentially', 'ADV', 'advmod']\n",
            "['unchanged', 'ADJ', 'acomp']\n",
            "['.', 'PUNCT', 'punct']\n",
            "['It', 'PRON', 'nsubjpass']\n",
            "['was', 'VERB', 'auxpass']\n",
            "['popularised', 'VERB', 'ROOT']\n",
            "['in', 'ADP', 'prep']\n",
            "['the', 'DET', 'det']\n",
            "['1960s', 'NOUN', 'pobj']\n",
            "['with', 'ADP', 'prep']\n",
            "['the', 'DET', 'det']\n",
            "['release', 'NOUN', 'pobj']\n",
            "['of', 'ADP', 'prep']\n",
            "['Letraset', 'PROPN', 'compound']\n",
            "['sheets', 'NOUN', 'pobj']\n",
            "['containing', 'VERB', 'acl']\n",
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'compound']\n",
            "['passages', 'NOUN', 'dobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['and', 'CCONJ', 'cc']\n",
            "['more', 'ADV', 'advmod']\n",
            "['recently', 'ADV', 'advmod']\n",
            "['with', 'ADP', 'conj']\n",
            "['desktop', 'ADJ', 'compound']\n",
            "['publishing', 'NOUN', 'compound']\n",
            "['software', 'NOUN', 'pobj']\n",
            "['like', 'ADP', 'prep']\n",
            "['Aldus', 'PROPN', 'compound']\n",
            "['PageMaker', 'PROPN', 'pobj']\n",
            "['including', 'VERB', 'prep']\n",
            "['versions', 'NOUN', 'pobj']\n",
            "['of', 'ADP', 'prep']\n",
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'pobj']\n",
            "['.', 'PUNCT', 'punct']\n",
            "['There', 'ADV', 'expl']\n",
            "['are', 'VERB', 'ROOT']\n",
            "['many', 'ADJ', 'amod']\n",
            "['variations', 'NOUN', 'attr']\n",
            "['of', 'ADP', 'prep']\n",
            "['passages', 'NOUN', 'pobj']\n",
            "['of', 'ADP', 'prep']\n",
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'pobj']\n",
            "['available', 'ADJ', 'amod']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['but', 'CCONJ', 'cc']\n",
            "['the', 'DET', 'det']\n",
            "['majority', 'NOUN', 'nsubj']\n",
            "['have', 'VERB', 'aux']\n",
            "['suffered', 'VERB', 'conj']\n",
            "['alteration', 'NOUN', 'dobj']\n",
            "['in', 'ADP', 'prep']\n",
            "['some', 'DET', 'det']\n",
            "['form', 'NOUN', 'pobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['by', 'ADP', 'prep']\n",
            "['injected', 'VERB', 'amod']\n",
            "['humour', 'NOUN', 'pobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['or', 'CCONJ', 'cc']\n",
            "['randomised', 'VERB', 'conj']\n",
            "['words', 'NOUN', 'dobj']\n",
            "['which', 'ADJ', 'nsubj']\n",
            "['do', 'VERB', 'aux']\n",
            "[\"n't\", 'ADV', 'neg']\n",
            "['look', 'VERB', 'relcl']\n",
            "['even', 'ADV', 'advmod']\n",
            "['slightly', 'ADV', 'advmod']\n",
            "['believable', 'ADJ', 'acomp']\n",
            "['.', 'PUNCT', 'punct']\n",
            "['If', 'ADP', 'mark']\n",
            "['you', 'PRON', 'nsubj']\n",
            "['are', 'VERB', 'aux']\n",
            "['going', 'VERB', 'advcl']\n",
            "['to', 'PART', 'aux']\n",
            "['use', 'VERB', 'xcomp']\n",
            "['a', 'DET', 'det']\n",
            "['passage', 'NOUN', 'dobj']\n",
            "['of', 'ADP', 'prep']\n",
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'pobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['you', 'PRON', 'nsubj']\n",
            "['need', 'VERB', 'ROOT']\n",
            "['to', 'PART', 'aux']\n",
            "['be', 'VERB', 'xcomp']\n",
            "['sure', 'ADJ', 'acomp']\n",
            "['there', 'ADV', 'expl']\n",
            "['is', 'VERB', 'ccomp']\n",
            "[\"n't\", 'ADV', 'neg']\n",
            "['anything', 'NOUN', 'attr']\n",
            "['embarrassing', 'VERB', 'amod']\n",
            "['hidden', 'VERB', 'acl']\n",
            "['in', 'ADP', 'prep']\n",
            "['the', 'DET', 'det']\n",
            "['middle', 'NOUN', 'pobj']\n",
            "['of', 'ADP', 'prep']\n",
            "['text', 'NOUN', 'pobj']\n",
            "['.', 'PUNCT', 'punct']\n",
            "['All', 'ADJ', 'predet']\n",
            "['the', 'DET', 'det']\n",
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'nsubj']\n",
            "['generators', 'NOUN', 'nsubj']\n",
            "['on', 'ADP', 'prep']\n",
            "['the', 'DET', 'det']\n",
            "['Internet', 'NOUN', 'pobj']\n",
            "['tend', 'VERB', 'ROOT']\n",
            "['to', 'PART', 'aux']\n",
            "['repeat', 'VERB', 'xcomp']\n",
            "['predefined', 'ADJ', 'amod']\n",
            "['chunks', 'NOUN', 'dobj']\n",
            "['as', 'ADP', 'prep']\n",
            "['necessary', 'ADJ', 'amod']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['making', 'VERB', 'advcl']\n",
            "['this', 'DET', 'nsubj']\n",
            "['the', 'DET', 'det']\n",
            "['first', 'ADJ', 'amod']\n",
            "['true', 'ADJ', 'amod']\n",
            "['generator', 'NOUN', 'ccomp']\n",
            "['on', 'ADP', 'prep']\n",
            "['the', 'DET', 'det']\n",
            "['Internet', 'NOUN', 'pobj']\n",
            "['.', 'PUNCT', 'punct']\n",
            "['It', 'PRON', 'nsubj']\n",
            "['uses', 'VERB', 'ROOT']\n",
            "['a', 'DET', 'det']\n",
            "['dictionary', 'NOUN', 'dobj']\n",
            "['of', 'ADP', 'prep']\n",
            "['over', 'ADP', 'quantmod']\n",
            "['200', 'NUM', 'nummod']\n",
            "['Latin', 'ADJ', 'amod']\n",
            "['words', 'NOUN', 'pobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['combined', 'VERB', 'acl']\n",
            "['with', 'ADP', 'prep']\n",
            "['a', 'DET', 'det']\n",
            "['handful', 'NOUN', 'pobj']\n",
            "['of', 'ADP', 'prep']\n",
            "['model', 'NOUN', 'compound']\n",
            "['sentence', 'NOUN', 'compound']\n",
            "['structures', 'NOUN', 'pobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['to', 'PART', 'aux']\n",
            "['generate', 'VERB', 'xcomp']\n",
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'dobj']\n",
            "['which', 'ADJ', 'nsubj']\n",
            "['looks', 'VERB', 'relcl']\n",
            "['reasonable', 'ADJ', 'acomp']\n",
            "['.', 'PUNCT', 'punct']\n",
            "['The', 'DET', 'det']\n",
            "['generated', 'VERB', 'amod']\n",
            "['Lorem', 'PROPN', 'compound']\n",
            "['Ipsum', 'PROPN', 'nsubj']\n",
            "['is', 'VERB', 'ROOT']\n",
            "['therefore', 'ADV', 'advmod']\n",
            "['always', 'ADV', 'advmod']\n",
            "['free', 'ADJ', 'acomp']\n",
            "['from', 'ADP', 'prep']\n",
            "['repetition', 'NOUN', 'pobj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['injected', 'VERB', 'amod']\n",
            "['humour', 'NOUN', 'conj']\n",
            "[',', 'PUNCT', 'punct']\n",
            "['or', 'CCONJ', 'cc']\n",
            "['non', 'ADJ', 'nmod']\n",
            "['-', 'PUNCT', 'punct']\n",
            "['characteristic', 'ADJ', 'amod']\n",
            "['words', 'NOUN', 'compound']\n",
            "['etc', 'X', 'conj']\n",
            "['.', 'PUNCT', 'punct']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqF1ydsLAPx8"
      },
      "source": [
        "from spacy import displacy\n",
        "nlp = spacy.load('en')\n",
        "doc1 = nlp(u'This is a sentence.')\n",
        "doc2 = nlp(u'This is another sentence.')\n",
        "#displacy.serve([doc1, doc2], style='dep')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANsJdsXoAPyA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "outputId": "9f4b5cca-08ec-4209-bc08-71c1327e77b1"
      },
      "source": [
        "displacy.render([doc1, doc2], jupyter=True, style='dep')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"536-0\" class=\"displacy\" width=\"750\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-536-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-536-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-536-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-536-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-536-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-536-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"536-1\" class=\"displacy\" width=\"750\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">another</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-536-1-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-536-1-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-536-1-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-536-1-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-536-1-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-536-1-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1foGvcs4APyD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "cfd7423a-3ed9-47ac-f445-593ee345e567"
      },
      "source": [
        "displacy.render(nlp(str(text)), jupyter=True, style='ent')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lorem Ipsum\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is simply dummy text of the printing and typesetting industry. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lorem Ipsum\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " has been the industry's standard dummy text ever since \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    the 1500s\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    only five centuries\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    the 1960s\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " with the release of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Letraset\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " sheets containing \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lorem Ipsum\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " passages, and more recently with desktop publishing software like \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Aldus PageMaker\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " including versions of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lorem Ipsum\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ".There are many variations of passages of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lorem Ipsum\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable. If you are going to use a passage of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lorem Ipsum\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", you need to be sure there isn't anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " true generator on the Internet. It uses a dictionary of over \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    200\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Latin\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " words, combined with a handful of model sentence structures, to generate \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lorem Ipsum\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " which looks reasonable. The generated \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lorem Ipsum\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is therefore always free from repetition, injected humour, or non-characteristic words etc.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IzEhEhjCJ5u"
      },
      "source": [
        "#!pip install spacymoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4lSwAiBAPyG"
      },
      "source": [
        "from spacymoji import Emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpx2p8_1APyJ"
      },
      "source": [
        "emoji = Emoji(nlp)\n",
        "nlp.add_pipe(emoji, first=True)\n",
        "\n",
        "doc = nlp(u'This is a test 😻 👍🏿')\n",
        "assert doc._.has_emoji == True\n",
        "assert doc[2:5]._.has_emoji == True\n",
        "assert doc[0]._.is_emoji == False\n",
        "assert doc[4]._.is_emoji == True\n",
        "assert doc[5]._.emoji_desc == u'thumbs up dark skin tone'\n",
        "assert len(doc._.emoji) == 2\n",
        "assert doc._.emoji[1] == (u'👍🏿', 5, u'thumbs up dark skin tone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BMj1L3wAPyL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0fa15dc-e738-487d-ceca-5ff405fbca7f"
      },
      "source": [
        "doc._.emoji[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('😻', 4, 'smiling cat face with heart-eyes')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcqBnob-DEDz"
      },
      "source": [
        "!pip install mordecai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_orC_1PDAPyP"
      },
      "source": [
        " from mordecai import Geoparser\n",
        " geo = Geoparser()\n",
        " geo.geoparse(\"I traveled from Oxford to Ottawa.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeeVQjoNAPyS"
      },
      "source": [
        "!pip install spacy_langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KvRZCJ4APyW"
      },
      "source": [
        "from spacy_langdetect import LanguageDetector\n",
        "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N9nysFCAPyY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "5412bb43-3009-45eb-bdd0-c53c9bd8028b"
      },
      "source": [
        "text5 = 'It was the 2nd to last day for us in Armenia. Already seen lot of places and had a brilliant time, but our trip with Gardman Tour was the best part of our trip. Hiking to mount Ara was totally worth it. Spectacular views and professional guiding. Without a doubt I would recommend this to anyone!'\n",
        "text6=\"Как передает Reuters, господин Трамп передал северокорейскому коллеге документ на корейском и английском языках в первый день саммита. Именно передачу всего оружия Вашингтон называет «окончательной и в полной мере верифицируемой денуклеаризацией Северной Кореи», указывает источник агентства.\"\n",
        "doc = nlp(text6)\n",
        "print(doc._.language)\n",
        "# sentence level language detection\n",
        "for sent in doc.sents:\n",
        "   print(sent, sent._.language)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'language': 'ru', 'score': 0.9999961027166694}\n",
            "Как передает Reuters, господин Трамп передал северокорейскому коллеге документ на корейском и английском языках в первый день саммита. Именно передачу всего оружия Вашингтон называет {'language': 'ru', 'score': 0.9999967490686383}\n",
            "«окончательной и в полной мере верифицируемой денуклеаризацией Северной Кореи», указывает источник агентства. {'language': 'ru', 'score': 0.9999980759797245}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-p0w6UtAPyb"
      },
      "source": [
        ""
      ]
    }
  ]
}